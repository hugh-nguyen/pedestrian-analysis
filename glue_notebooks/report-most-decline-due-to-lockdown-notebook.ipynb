{
	"metadata": {
		"kernelspec": {
			"name": "glue_pyspark",
			"display_name": "Glue PySpark",
			"language": "python"
		},
		"language_info": {
			"name": "Python_Glue_Session",
			"mimetype": "text/x-python",
			"codemirror_mode": {
				"name": "python",
				"version": 3
			},
			"pygments_lexer": "python3",
			"file_extension": ".py"
		}
	},
	"nbformat_minor": 4,
	"nbformat": 4,
	"cells": [
		{
			"cell_type": "markdown",
			"source": "# Locations with Most Decline Due to Lockdown",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "markdown",
			"source": "####  Import packages and start the session",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "%idle_timeout 2880\n%glue_version 3.0\n%worker_type G.1X\n%number_of_workers 2\n%extra_py_files s3://pedestrian-analysis-working-bucket/glue-job-scripts/util.py\n\nimport sys, io, util\nfrom datetime import datetime, timedelta\n\nfrom pyspark.sql.types import (\n    StructType, StructField, StringType, IntegerType, DoubleType, TimestampType\n)\n\nfrom pyspark.context import SparkContext\nfrom pyspark.sql.functions import sum, col, rank, desc, lit, when\nfrom pyspark.sql.window import Window\nfrom awsglue.utils import getResolvedOptions\nfrom awsglue.context import GlueContext\nfrom awsglue.job import Job\n\nsc = SparkContext.getOrCreate()\nglueContext = GlueContext(sc)\nspark = glueContext.spark_session\njob = Job(glueContext)",
			"metadata": {
				"editable": true,
				"trusted": true
			},
			"execution_count": 1,
			"outputs": [
				{
					"name": "stdout",
					"text": "Welcome to the Glue Interactive Sessions Kernel\nFor more information on available magic commands, please type %help in any new cell.\n\nPlease view our Getting Started page to access the most up-to-date information on the Interactive Sessions kernel: https://docs.aws.amazon.com/glue/latest/dg/interactive-sessions.html\nInstalled kernel version: 0.37.0 \nCurrent idle_timeout is 2800 minutes.\nidle_timeout has been set to 2880 minutes.\nSetting Glue version to: 3.0\nPrevious worker type: G.1X\nSetting new worker type to: G.1X\nPrevious number of workers: 5\nSetting new number of workers to: 2\nExtra py files to be included:\ns3://pedestrian-analysis-working-bucket/glue-job-scripts/util.py\nAuthenticating with environment variables and user-defined glue_role_arn: arn:aws:iam::632753217422:role/pedestrians-analysis-notebook-role\nTrying to create a Glue session for the kernel.\nWorker Type: G.1X\nNumber of Workers: 2\nSession ID: ea39848e-0a69-45c9-b6d3-16712af8714e\nJob Type: glueetl\nApplying the following default arguments:\n--glue_kernel_version 0.37.0\n--enable-glue-datacatalog true\n--extra-py-files s3://pedestrian-analysis-working-bucket/glue-job-scripts/util.py\nWaiting for session ea39848e-0a69-45c9-b6d3-16712af8714e to get into ready status...\nSession ea39848e-0a69-45c9-b6d3-16712af8714e has been created.\n\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "####  Create output glue table if it doesn't already exist\n##### The results of this notebook will be loaded into this table",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "BUCKET_NAME = 'pedestrian-analysis-working-bucket'\nDATABASE_NAME = 'pedestrian_analysis_report'\nOUTPUT_TABLE_NAME = 'report_location_declines_due_to_lockdown'\n\nschema = StructType([\n    StructField(\"location_name\", StringType(), True),\n    StructField(\"2019_count\", IntegerType(), True),\n    StructField(\"2022_count\", IntegerType(), True),\n    StructField(\"decline\", IntegerType(), True),\n    StructField(\"decline_percent\", DoubleType(), True),\n])\n\ns3_path = f\"s3://{BUCKET_NAME}/report/{OUTPUT_TABLE_NAME}/\"\nutil.create_glue_catalog_table(DATABASE_NAME, OUTPUT_TABLE_NAME, schema, s3_path)",
			"metadata": {
				"editable": true,
				"trusted": true
			},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "markdown",
			"source": "####  Load sensor_counts_by_day",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "sensor_counts_df = glueContext.create_dynamic_frame.from_catalog(\n    database=\"pedestrian_analysis_raw\",\n    table_name=\"sensor_counts\"\n).toDF()\n\nsensor_counts_df.show(10, truncate=False)",
			"metadata": {
				"editable": true,
				"trusted": true
			},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "markdown",
			"source": "####  Load sensor_reference_data",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "sensor_reference_df = glueContext.create_dynamic_frame.from_catalog(\n    database=\"pedestrian_analysis_raw\",\n    table_name=\"sensor_reference_data\"\n).toDF()\n\nsensor_reference_df.show(10)",
			"metadata": {
				"editable": true,
				"trusted": true
			},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "markdown",
			"source": "#### Calculate sensor counts 2019\n##### Because the cutsoff at 2022-11-01, for the purpose of this analysis we are setting the end of each year to November 11",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "sensor_counts_2019_df = sensor_counts_df \\\n    .filter(col('date_time') >= '2018-11-01') \\\n    .filter(col('date_time') < '2019-11-01') \\\n    .groupBy('sensor_id') \\\n    .agg(sum('hourly_count').alias('count_2019'))\n\nsensor_counts_2019_df.show(10)",
			"metadata": {
				"editable": true,
				"trusted": true
			},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "markdown",
			"source": "#### Calculate sensor counts 2022",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "sensor_counts_2022_df = sensor_counts_df \\\n    .filter(col('date_time') >= '2021-11-01') \\\n    .filter(col('date_time') < '2022-11-01') \\\n    .groupBy('sensor_id') \\\n    .agg(sum('hourly_count').alias('count_2022'))\n\nsensor_counts_2022_df.show(10)",
			"metadata": {
				"trusted": true
			},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "markdown",
			"source": "#### Calculate the decline and decline percentages for each sensor",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "sensor_decline_df = sensor_counts_2019_df \\\n    .join(sensor_counts_2022_df, on='sensor_id', how='inner') \\\n    .withColumn('decline', (col('count_2019') - col('count_2022'))) \\\n    .withColumn('decline_percent', ((col('count_2019') - col('count_2022')) / col('count_2019')) * 100)\n\nsensor_decline_df.show(10)",
			"metadata": {
				"trusted": true
			},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "#### Join reference and select relevant columns",
			"metadata": {
				"trusted": true
			},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "sensor_decline_df = sensor_decline_df.join(\n    sensor_reference_df,\n    col(\"sensor_id\") == col(\"location_id\"),\n    \"left\"\n)\n\nsensor_decline_df = sensor_decline_df.select(\n    col('sensor_id'),\n    col('sensor_description').alias('location_name'),\n    col('count_2019').cast('int'),\n    col('count_2022').cast('int'),\n    col('decline'),\n    col('decline_percent')\n).orderBy(\n    desc('decline_percent')\n)\n\nsensor_decline_df.show(100, truncate=False)",
			"metadata": {
				"trusted": true
			},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "util.upload_to_s3(glueContext, sensor_decline_df, s3_path)",
			"metadata": {
				"trusted": true
			},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		}
	]
}